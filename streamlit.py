import os
import re
import time
import unicodedata
import requests
import pandas as pd
from bs4 import BeautifulSoup
from io import BytesIO, StringIO
import streamlit as st

# ===== ê³ ì • ì„¤ì • =====
BASE = "https://www.adiga.kr"
DETAIL_URL = f"{BASE}/ucp/uvt/uni/univDetail.do"
DOWNLOAD_URL = f"{BASE}/cmm/com/file/fileDown.do"
MENU_ID = "PCUVTINF2000"
SEARCH_YEAR_DEFAULT = 2026

# ===== ìœ í‹¸ í•¨ìˆ˜ =====
def sanitize_filename(name: str) -> str:
    name = unicodedata.normalize("NFKC", str(name))
    name = re.sub(r'[<>:"/\\|?*\x00-\x1F]', ' ', name)
    name = re.sub(r'\s+', ' ', name).strip()
    return name

def norm_text(el) -> str:
    return ' '.join(el.get_text(separator=' ', strip=True).split())

def wrap_long_text(df, max_len=50):
    df_wrapped = df.copy()
    for col in df_wrapped.columns:
        df_wrapped[col] = df_wrapped[col].apply(
            lambda x: "\n".join([str(x)[i:i+max_len] for i in range(0, len(str(x)), max_len)])
        )
    return df_wrapped

# ===== ì „í˜•ë³„ ì½”ë“œ =====
types_results = {
    "í•™ìƒë¶€ì¢…í•©": {"upcd": "20", "cd": "22"},
    "í•™ìƒë¶€êµê³¼": {"upcd": "30", "cd": "32"},
    "ìˆ˜ëŠ¥": {"upcd": "40", "cd": "42"},
}
types_main = {
    "í•™ìƒë¶€ì¢…í•©(ì£¼ìš”ì‚¬í•­)": {"upcd": "20", "cd": "21"},
    "í•™ìƒë¶€êµê³¼(ì£¼ìš”ì‚¬í•­)": {"upcd": "30", "cd": "31"},
    "ìˆ˜ëŠ¥(ì£¼ìš”ì‚¬í•­)": {"upcd": "40", "cd": "41"},
}

# ===== ìš”ì²­ í—¤ë” / ì¿ í‚¤ =====
cookies = {
    'WMONID': 'NYfDEAkX3Jy',
    'JSESSIONID': 'V9Tor4qz9JI1R0wOWXqKXhcJbeLiyXWdTSgfWj1hzo1aRGbUlCTAoSQSWOuxxFFK.amV1c19kb21haW4vYWRpZ2Ex',
}
headers = {
    'Accept': 'application/json, text/plain, */*',
    'Content-Type': 'application/x-www-form-urlencoded',
    'Origin': 'https://www.adiga.kr',
    'Referer': 'https://www.adiga.kr/uct/acd/ade/criteriaAndResultPopup.do',
    'User-Agent': 'Mozilla/5.0',
    'X-CSRF-TOKEN': 'b4561457-4e76-449b-909b-9099-c36118c3f560',
    'X-Requested-With': 'XMLHttpRequest',
}

# ===== Streamlit UI =====
st.set_page_config(layout="wide")
st.title("ëŒ€í•™ ì…ì‹œìë£Œ ì¡°íšŒ ë° ë‹¤ìš´ë¡œë“œ")

# ===== GitHubì—ì„œ ëŒ€í•™ ëª©ë¡ ë¡œë“œ =====
@st.cache_data(show_spinner=False)
def load_university_list(github_url):
    response = requests.get(github_url)
    response.raise_for_status()
    file_bytes = BytesIO(response.content)
    df = pd.read_excel(file_bytes, engine='openpyxl')
    df = df.dropna(subset=[df.columns[0], df.columns[1]])
    return df

# GitHub íŒŒì¼ URL
GITHUB_URL = "https://raw.githubusercontent.com/ì‚¬ìš©ìëª…/ì €ì¥ì†Œëª…/ë¸Œëœì¹˜/ëŒ€í•™êµë³„ ì½”ë“œ.xlsx"

df = load_university_list(GITHUB_URL)
univ_list = df["í•™êµëª…"].tolist()

# ì‚¬ì´ë“œë°”
with st.sidebar:
    search_year = st.number_input("í•™ë…„ë„ ì…ë ¥", min_value=2000, max_value=2100, value=SEARCH_YEAR_DEFAULT, step=1)
    selected_univ = st.selectbox("ëŒ€í•™ ì„ íƒ", univ_list)
    types_options = ["ì „ì²´"] + list(types_results.keys()) + list(types_main.keys())
    selected_type = st.selectbox("ì „í˜• ì„ íƒ", types_options)

# ===== ëª¨ì§‘ìš”ê°• PDF ë‹¤ìš´ë¡œë“œ =====
@st.cache_data(show_spinner=False)
def extract_and_download_pdfs(unv_cd, search_syr, univ_name):
    plan_ids = susi_ids = jeongsi_ids = None
    params = {"menuId": MENU_ID, "unvCd": unv_cd, "searchSyr": search_syr}
    headers_req = {"User-Agent": "Mozilla/5.0"}
    res = requests.get(DETAIL_URL, params=params, headers=headers_req, timeout=30)
    res.raise_for_status()
    soup = BeautifulSoup(res.text, "html.parser")
    ul = soup.select_one("ul#fileResult")
    if ul:
        for li in ul.select("li"):
            a = li.select_one("a[onclick]")
            span = li.select_one("span")
            if not a or not span:
                continue
            text = norm_text(span)
            onclick = a.get("onclick", "")
            m = re.search(r"fnUnvFileDownOne\(\s*'([^']+)'\s*,\s*'([^']+)'\s*,", onclick)
            if not m:
                continue
            file_id, file_sn = m.group(1), m.group(2)
            if ("ëŒ€í•™ì…í•™ì „í˜•" in text) and ("ì‹œí–‰ê³„íš" in text):
                plan_ids = (file_id, file_sn)
            elif ("ìˆ˜ì‹œ" in text) and ("ëª¨ì§‘ìš”ê°•" in text):
                susi_ids = (file_id, file_sn)
            elif ("ì •ì‹œ" in text) and ("ëª¨ì§‘ìš”ê°•" in text):
                jeongsi_ids = (file_id, file_sn)
    pdf_buffers = {}
    for label, ids in [("ì‹œí–‰ê³„íš", plan_ids), ("ìˆ˜ì‹œ", susi_ids), ("ì •ì‹œ", jeongsi_ids)]:
        if ids:
            f_id, f_sn = ids
            params_file = {
                "fileId": f_id,
                "fileSn": f_sn,
                "menuId": MENU_ID,
                "downLogYn": "Y",
                "unvCd": unv_cd,
                "searchSyr": search_syr,
                "_": str(int(time.time() * 1000)),
            }
            headers_file = {
                "User-Agent": "Mozilla/5.0",
                "Referer": f"{DETAIL_URL}?menuId={MENU_ID}&unvCd={unv_cd}&searchSyr={search_syr}",
                "X-Requested-With": "XMLHttpRequest",
            }
            r = requests.get(DOWNLOAD_URL, params=params_file, headers=headers_file, timeout=60)
            if r.status_code == 200:
                fname = sanitize_filename(f"{univ_name}_{label}_ëª¨ì§‘ìš”ê°•.pdf")
                pdf_buffers[label] = (r.content, fname)
    return pdf_buffers

# ===== ì „í˜•ë³„ ì…ì‹œìë£Œ í¬ë¡¤ë§ =====
def crawl_admission_result_single(unv_cd, search_syr, sheet_name):
    if sheet_name in types_main:
        codes = types_main[sheet_name]
    elif sheet_name in types_results:
        codes = types_results[sheet_name]
    else:
        return None
    data = {
        '_csrf': headers['X-CSRF-TOKEN'],
        'searchSyr': search_syr,
        'unvCd': str(unv_cd).zfill(7),
        'compUnvCd': '',
        'searchUnvComp': '0',
        'tsrdCmphSlcnArtclUpCd': codes['upcd'],
        'tsrdCmphSlcnArtclCd': codes['cd'],
    }
    try:
        response = requests.post(
            'https://www.adiga.kr/uct/acd/ade/criteriaAndResultItemAjax.do',
            cookies=cookies, headers=headers, data=data, timeout=30
        )
        time.sleep(0.2)
        soup = BeautifulSoup(response.text, 'lxml')
        tables = soup.find_all('table')
        df_list = []
        for table in tables:
            try:
                df_table = pd.read_html(StringIO(str(table)), flavor='lxml')[0]
                df_list.append(df_table)
                df_list.append(pd.DataFrame([['' for _ in range(df_table.shape[1])]]))
            except:
                continue
        if df_list:
            combined_df = pd.concat(df_list, ignore_index=True)
            return combined_df
        else:
            return None
    except Exception as e:
        st.warning(f"{sheet_name} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        return None

# ===== ë²„íŠ¼ í´ë¦­ í›„ í¬ë¡¤ë§ ì‹œì‘ =====
if st.button("í¬ë¡¤ë§ ì‹œì‘"):
    row = df[df["í•™êµëª…"] == selected_univ].iloc[0]
    unv_cd = str(row["ì½”ë“œë²ˆí˜¸"]).zfill(7)

    st.info(f"{selected_univ} ì…ì‹œìë£Œ ë¡œë”© ì¤‘... â³")

    # PDF ë‹¤ìš´ë¡œë“œ
    pdf_buffers = extract_and_download_pdfs(unv_cd, search_year, selected_univ)

    # ===== ì˜¤ë¥¸ìª½ í™”ë©´ ìƒ/í•˜ í”„ë ˆì„ =====
    top_container = st.container()   # ì£¼ìš”ì‚¬í•­
    bottom_container = st.container() # ì…ì‹œê²°ê³¼

    # ìƒë‹¨: ì£¼ìš”ì‚¬í•­
    st.subheader(f"ğŸ“Œ {search_year} ì „í˜•ë³„ ì£¼ìš”ì‚¬í•­")
    for sheet_name in types_main.keys():
        if selected_type != "ì „ì²´" and selected_type != sheet_name:
            continue
        placeholder = top_container.empty()
        df_sheet = crawl_admission_result_single(unv_cd, search_year, sheet_name)
        if df_sheet is not None:
            df_to_show = wrap_long_text(df_sheet, max_len=50)
            placeholder.markdown(f"**{sheet_name}**")
            placeholder.dataframe(df_to_show, use_container_width=True)

    # í•˜ë‹¨: ì…ì‹œê²°ê³¼
    st.subheader(f"ğŸ“Š {search_year-1}í•™ë…„ë„ ì…ì‹œê²°ê³¼")
    for sheet_name in types_results.keys():
        if selected_type != "ì „ì²´" and selected_type != sheet_name:
            continue
        placeholder = bottom_container.empty()
        df_sheet = crawl_admission_result_single(unv_cd, search_year, sheet_name)
        if df_sheet is not None:
            placeholder.markdown(f"**{sheet_name}**")
            placeholder.dataframe(df_sheet, use_container_width=True)

    # Excel ë‹¤ìš´ë¡œë“œ
    excel_buffer = BytesIO()
    with pd.ExcelWriter(excel_buffer, engine="openpyxl") as writer:
        for sheet_name in list(types_main.keys()) + list(types_results.keys()):
            if selected_type != "ì „ì²´" and selected_type != sheet_name:
                continue
            df_sheet = crawl_admission_result_single(unv_cd, search_year, sheet_name)
            if df_sheet is not None:
                df_sheet.to_excel(writer, sheet_name=sheet_name[:31], index=False, header=False)
    excel_buffer.seek(0)
    st.download_button(
        label="ğŸ“¥ ì…ì‹œê²°ê³¼ ë‹¤ìš´ë¡œë“œ",
        data=excel_buffer,
        file_name=f"{sanitize_filename(selected_univ)}_{search_year-1}ë…„_ëŒ€í•™ì…ì‹œê²°ê³¼.xlsx",
        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
    )

    # PDF ë‹¤ìš´ë¡œë“œ
    if pdf_buffers:
        st.markdown("### ëª¨ì§‘ìš”ê°• PDF ë‹¤ìš´ë¡œë“œ")
        for label, (content, fname) in pdf_buffers.items():
            st.download_button(
                label=f"ğŸ“„ {label} ë‹¤ìš´ë¡œë“œ",
                data=content,
                file_name=fname,
                mime="application/pdf"
            )
    else:
        st.warning("ëª¨ì§‘ìš”ê°• PDFê°€ ì—†ìŠµë‹ˆë‹¤.")

    st.success("í¬ë¡¤ë§ ì™„ë£Œ! âœ…")

import streamlit as st
import pandas as pd
import requests
from bs4 import BeautifulSoup
import time
from io import BytesIO

# --- ì „í˜• ì½”ë“œ ë§¤í•‘ ---
types = {
    "í•™ìƒë¶€ì¢…í•©": {"upcd": "20", "cd": "22"},
    "í•™ìƒë¶€êµê³¼": {"upcd": "30", "cd": "32"},
    "ìˆ˜ëŠ¥": {"upcd": "40", "cd": "42"},
}

# --- ëŒ€í•™ ëª©ë¡ ë¶ˆëŸ¬ì˜¤ê¸° (GitHub URL ì‚¬ìš©) ---
@st.cache_data
def load_university_list(github_url):
    try:
        response = requests.get(github_url)
        response.raise_for_status()
        file_bytes = BytesIO(response.content)
        df = pd.read_excel(file_bytes, engine='openpyxl')
        df = df.dropna(subset=[df.columns[0], df.columns[1]])
        return df.values.tolist()
    except requests.exceptions.RequestException as e:
        st.error(f"GitHubì—ì„œ íŒŒì¼ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")
        return None

# --- í¬ë¡¤ë§ í•¨ìˆ˜: ì—¬ëŸ¬ í…Œì´ë¸” ì²˜ë¦¬ ë¡œì§ ---
def crawl_admission_result(univ_name, univ_code, selected_types):
    cookies = {
        'WMONID': 'NYfDEAkX3Jy',
        'JSESSIONID': 'V9Tor4qz9JI1R0wOWXqKXhcJbeLiyXWdTSgfWj1hzo1aRGzUlCTAoSQSWOuxxFFK.amV1c19kb21haW4vYWRpZ2Ex',
    }
    headers = {
        'Accept': 'application/json, text/plain, */*',
        'Accept-Language': 'ko-KR,ko;q=0.9',
        'Content-Type': 'application/x-www-form-urlencoded',
        'Origin': 'https://www.adiga.kr',
        'Referer': 'https://www.adiga.kr/uct/acd/ade/criteriaAndResultPopup.do',
        'User-Agent': 'Mozilla/5.0',
        'X-CSRF-TOKEN': 'b4561457-4e76-449b-9099-c36118c3f560',
        'X-Requested-With': 'XMLHttpRequest',
    }

    sheet_data = {}

    for name in selected_types:
        all_data = []
        codes = types.get(name)
        if not codes:
            continue
        data = {
            '_csrf': 'b4561457-4e76-449b-9099-c36118c3f560',
            'searchSyr': '2026',
            'unvCd': str(univ_code).zfill(7),
            'searchUnvComp': '0',
            'tsrdCmphSlcnArtclUpCd': codes['upcd'],
            'tsrdCmphSlcnArtclCd': codes['cd'],
        }
        response = requests.post(
            'https://www.adiga.kr/uct/acd/ade/criteriaAndResultItemAjax.do',
            cookies=cookies,
            headers=headers,
            data=data,
        )
        time.sleep(0.5)

        soup = BeautifulSoup(response.text, 'html.parser')
        tables = soup.select("table")
        if not tables:
            continue

        for table in tables:
            span_map = {}
            table_matrix = []
            rows = table.find_all('tr')
            for row_idx, row in enumerate(rows):
                cells = row.find_all(['td', 'th'])
                current_row = [None] * 100
                col_idx = 0
                for cell in cells:
                    while (row_idx, col_idx) in span_map:
                        current_row[col_idx] = span_map[(row_idx, col_idx)][2]
                        col_idx += 1
                    text = cell.get_text(strip=True)
                    rowspan = int(cell.get('rowspan', 1))
                    colspan = int(cell.get('colspan', 1))
                    for r in range(rowspan):
                        for c in range(colspan):
                            if r == 0 and c == 0:
                                current_row[col_idx] = text
                            else:
                                span_map[(row_idx + r, col_idx + c)] = (rowspan, colspan, text)
                    col_idx += colspan
                table_matrix.append(current_row[:col_idx])
            df = pd.DataFrame(table_matrix).fillna('')
            all_data.append(df)
            all_data.append(pd.DataFrame([['' for _ in range(df.shape[1])]]))

        if all_data:
            combined = pd.concat(all_data, ignore_index=True)
            sheet_data[name] = combined

    return sheet_data

# --- Streamlit ì•± ì‹œì‘ ---
st.title("ğŸ“ 2025 ëŒ€í•™ ì…ì‹œ ê²°ê³¼ í¬ë¡¤ë§")

GITHUB_RAW_URL = "https://raw.githubusercontent.com/fresh601/university_crawlig/main/ëŒ€í•™êµë³„%20ì½”ë“œ.xlsx"
univ_list = load_university_list(GITHUB_RAW_URL)

if univ_list is not None:
    univ_dict = {name: code for name, code in univ_list}

    selected_univ = st.selectbox("ğŸ« ëŒ€í•™ ì„ íƒ", list(univ_dict.keys()))
    selected_types = st.multiselect("ğŸ“Œ ì „í˜• ì„ íƒ", list(types.keys()), default=["í•™ìƒë¶€ì¢…í•©"])

    if st.button("ğŸ“Š ì…ì‹œê²°ê³¼ ë¶ˆëŸ¬ì˜¤ê¸°"):
        with st.spinner(f"{selected_univ}ì˜ ì…ì‹œê²°ê³¼ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
            sheet_data = crawl_admission_result(selected_univ, univ_dict[selected_univ], selected_types)

            if not sheet_data:
                st.warning("í•´ë‹¹ ëŒ€í•™ì˜ ì„ íƒí•œ ì „í˜• ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            else:
                output = BytesIO()
                with pd.ExcelWriter(output, engine='openpyxl') as writer:
                    for sheet_name, df in sheet_data.items():
                        df.to_excel(writer, sheet_name=sheet_name, index=False, header=False)
                st.success("ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ!")
                st.download_button(
                    label="ğŸ“¥ ì—‘ì…€ íŒŒì¼ ë‹¤ìš´ë¡œë“œ",
                    data=output.getvalue(),
                    file_name=f"{selected_univ}_2025ë…„_ëŒ€í•™ì…ì‹œê²°ê³¼.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                )

                for sheet_name, df in sheet_data.items():
                    st.subheader(f"ğŸ“„ {sheet_name}")
                    st.dataframe(df)
else:
    st.info("GitHubì—ì„œ 'ëŒ€í•™êµë³„ ì½”ë“œ.xlsx' íŒŒì¼ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. URLì„ í™•ì¸í•´ ì£¼ì„¸ìš”.")




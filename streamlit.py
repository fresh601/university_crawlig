import os
import re
import time
import unicodedata
import requests
import pandas as pd
from bs4 import BeautifulSoup
from io import BytesIO, StringIO
import streamlit as st

# ===== ê³ ì • ì„¤ì • =====
BASE = "https://www.adiga.kr"
DETAIL_URL = f"{BASE}/ucp/uvt/uni/univDetail.do"
DOWNLOAD_URL = f"{BASE}/cmm/com/file/fileDown.do"
MENU_ID = "PCUVTINF2000"
SEARCH_YEAR_DEFAULT = 2026
UNIV_LIST_PATH = "ëŒ€í•™êµë³„ ì½”ë“œ.xlsx"  # ê¹ƒí—ˆë¸Œì— í¬í•¨ë  íŒŒì¼

# ===== ìœ í‹¸ í•¨ìˆ˜ =====
def sanitize_filename(name: str) -> str:
    name = unicodedata.normalize("NFKC", str(name))
    name = re.sub(r'[<>:"/\\|?*\x00-\x1F]', ' ', name)
    name = re.sub(r'\s+', ' ', name).strip()
    return name

def norm_text(el) -> str:
    return ' '.join(el.get_text(separator=' ', strip=True).split())

def wrap_long_text(df, max_len=50):
    df_wrapped = df.copy()
    for col in df_wrapped.columns:
        df_wrapped[col] = df_wrapped[col].apply(
            lambda x: "\n".join([str(x)[i:i+max_len] for i in range(0, len(str(x)), max_len)])
        )
    return df_wrapped

# ===== ì…ì‹œê²°ê³¼ í¬ë¡¤ë§ =====
cookies = {
    'WMONID': 'NYfDEAkX3Jy',
    'JSESSIONID': 'V9Tor4qz9JI1R0wOWXqKXhcJbeLiyXWdTSgfWj1hzo1aRGbUlCTAoSQSWOuxxFFK.amV1c19kb21haW4vYWRpZ2Ex',
}
headers = {
    'Accept': 'application/json, text/plain, */*',
    'Content-Type': 'application/x-www-form-urlencoded',
    'Origin': 'https://www.adiga.kr',
    'Referer': 'https://www.adiga.kr/uct/acd/ade/criteriaAndResultPopup.do',
    'User-Agent': 'Mozilla/5.0',
    'X-CSRF-TOKEN': 'b4561457-4e76-449b-909b-9099-c36118c3f560',
    'X-Requested-With': 'XMLHttpRequest',
}

types_results = {
    "í•™ìƒë¶€ì¢…í•©": {"upcd": "20", "cd": "22"},
    "í•™ìƒë¶€êµê³¼": {"upcd": "30", "cd": "32"},
    "ìˆ˜ëŠ¥": {"upcd": "40", "cd": "42"},
}
types_main = {
    "í•™ìƒë¶€ì¢…í•©(ì£¼ìš”ì‚¬í•­)": {"upcd": "20", "cd": "21"},
    "í•™ìƒë¶€êµê³¼(ì£¼ìš”ì‚¬í•­)": {"upcd": "30", "cd": "31"},
    "ìˆ˜ëŠ¥(ì£¼ìš”ì‚¬í•­)": {"upcd": "40", "cd": "41"},
}

def crawl_admission_results_chunk(unv_cd, search_syr, name, codes):
    sheet_data = {}
    data = {
        '_csrf': headers['X-CSRF-TOKEN'],
        'searchSyr': search_syr,
        'unvCd': str(unv_cd).zfill(7),
        'compUnvCd': '',
        'searchUnvComp': '0',
        'tsrdCmphSlcnArtclUpCd': codes['upcd'],
        'tsrdCmphSlcnArtclCd': codes['cd'],
    }
    try:
        response = requests.post(
            'https://www.adiga.kr/uct/acd/ade/criteriaAndResultItemAjax.do',
            cookies=cookies, headers=headers, data=data, timeout=30
        )
        time.sleep(0.2)
        soup = BeautifulSoup(response.text, 'lxml')
        tables = soup.find_all('table')
        df_list = []
        for table in tables:
            try:
                df_table = pd.read_html(StringIO(str(table)), flavor='lxml')[0]
                df_list.append(df_table)
                df_list.append(pd.DataFrame([['' for _ in range(df_table.shape[1])]]))
            except:
                continue
        if df_list:
            combined_df = pd.concat(df_list, ignore_index=True)
            sheet_data[name] = combined_df
    except Exception as e:
        st.warning(f"{name} í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
    return sheet_data

def extract_and_download_pdfs(unv_cd, search_syr, univ_name):
    plan_ids = susi_ids = jeongsi_ids = None
    params = {"menuId": MENU_ID, "unvCd": unv_cd, "searchSyr": search_syr}
    headers_req = {"User-Agent": "Mozilla/5.0"}
    res = requests.get(DETAIL_URL, params=params, headers=headers_req, timeout=30)
    res.raise_for_status()
    soup = BeautifulSoup(res.text, "html.parser")
    ul = soup.select_one("ul#fileResult")
    if ul:
        for li in ul.select("li"):
            a = li.select_one("a[onclick]")
            span = li.select_one("span")
            if not a or not span:
                continue
            text = norm_text(span)
            onclick = a.get("onclick", "")
            m = re.search(r"fnUnvFileDownOne\(\s*'([^']+)'\s*,\s*'([^']+)'\s*,", onclick)
            if not m:
                continue
            file_id, file_sn = m.group(1), m.group(2)
            if ("ëŒ€í•™ì…í•™ì „í˜•" in text) and ("ì‹œí–‰ê³„íš" in text):
                plan_ids = (file_id, file_sn)
            elif ("ìˆ˜ì‹œ" in text) and ("ëª¨ì§‘ìš”ê°•" in text):
                susi_ids = (file_id, file_sn)
            elif ("ì •ì‹œ" in text) and ("ëª¨ì§‘ìš”ê°•" in text):
                jeongsi_ids = (file_id, file_sn)
    pdf_buffers = {}
    for label, ids in [("ì‹œí–‰ê³„íš", plan_ids), ("ìˆ˜ì‹œ", susi_ids), ("ì •ì‹œ", jeongsi_ids)]:
        if ids:
            f_id, f_sn = ids
            params_file = {
                "fileId": f_id,
                "fileSn": f_sn,
                "menuId": MENU_ID,
                "downLogYn": "Y",
                "unvCd": unv_cd,
                "searchSyr": search_syr,
                "_": str(int(time.time() * 1000)),
            }
            headers_file = {
                "User-Agent": "Mozilla/5.0",
                "Referer": f"{DETAIL_URL}?menuId={MENU_ID}&unvCd={unv_cd}&searchSyr={search_syr}",
                "X-Requested-With": "XMLHttpRequest",
            }
            r = requests.get(DOWNLOAD_URL, params=params_file, headers=headers_file, timeout=60)
            if r.status_code == 200:
                fname = sanitize_filename(f"{univ_name}_{label}_ëª¨ì§‘ìš”ê°•.pdf")
                pdf_buffers[label] = (r.content, fname)
    return pdf_buffers

# ===== Streamlit UI =====
st.set_page_config(layout="wide")
st.title("ëŒ€í•™ ì…ì‹œìë£Œ ì¡°íšŒ ë° ë‹¤ìš´ë¡œë“œ")

if not os.path.exists(UNIV_LIST_PATH):
    st.error(f"{UNIV_LIST_PATH} íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ê¹ƒí—ˆë¸Œì— í¬í•¨ì‹œì¼œì£¼ì„¸ìš”.")
else:
    df = pd.read_excel(UNIV_LIST_PATH)
    if "ì½”ë“œë²ˆí˜¸" not in df.columns or "í•™êµëª…" not in df.columns:
        st.error("'ì½”ë“œë²ˆí˜¸'ì™€ 'í•™êµëª…' ì—´ì´ í•„ìš”í•©ë‹ˆë‹¤.")
    else:
        univ_list = df["í•™êµëª…"].tolist()

        # ì‚¬ì´ë“œë°”
        with st.sidebar:
            search_year = st.number_input("í•™ë…„ë„ ì…ë ¥", min_value=2000, max_value=2100,
                                          value=SEARCH_YEAR_DEFAULT, step=1)
            selected_univ = st.selectbox("ëŒ€í•™ ì„ íƒ", univ_list)
            types_options = ["ì „ì²´"] + list(types_results.keys()) + list(types_main.keys())
            selected_type = st.selectbox("ì „í˜• ì„ íƒ", types_options)

        # ===== Placeholder ì¤€ë¹„ =====
        top_container = st.container()   # ì£¼ìš”ì‚¬í•­
        bottom_container = st.container() # ì…ì‹œê²°ê³¼
        pdf_container = st.container()    # PDF ë‹¤ìš´ë¡œë“œ
        status_placeholder = st.empty()   # ìƒíƒœ ë©”ì‹œì§€ placeholder
        progress_bar = st.progress(0)

        if st.button("í¬ë¡¤ë§ ì‹œì‘") or "admission_data" in st.session_state:

            if "admission_data" not in st.session_state:
                row = df[df["í•™êµëª…"] == selected_univ].iloc[0]
                unv_cd = str(row["ì½”ë“œë²ˆí˜¸"]).zfill(7)
                st.session_state.admission_data = {}
                st.session_state.pdf_buffers = {}

                all_types = {**types_results, **types_main}
                total = len(all_types)
                for i, (name, codes) in enumerate(all_types.items(), 1):
                    status_placeholder.info(f"{name} í¬ë¡¤ë§ ì¤‘... ({i}/{total})")
                    data_chunk = crawl_admission_results_chunk(unv_cd, search_year, name, codes)
                    st.session_state.admission_data.update(data_chunk)

                    progress_bar.progress(i / total)

                status_placeholder.info("PDF í¬ë¡¤ë§ ì¤‘...")
                st.session_state.pdf_buffers = extract_and_download_pdfs(unv_cd, search_year, selected_univ)

            # ===== í™”ë©´ í‘œì‹œ =====
            with top_container:
                if any("ì£¼ìš”ì‚¬í•­" in name for name in st.session_state.admission_data.keys()):
                    st.header("ì£¼ìš”ì‚¬í•­")
                    for sheet_name, df_sheet in st.session_state.admission_data.items():
                        if "ì£¼ìš”ì‚¬í•­" not in sheet_name:
                            continue
                        if selected_type != "ì „ì²´" and selected_type != sheet_name:
                            continue
                        st.markdown(f"**{sheet_name}**")
                        st.dataframe(wrap_long_text(df_sheet, max_len=50), use_container_width=True)

            with bottom_container:
                if any("ì£¼ìš”ì‚¬í•­" not in name for name in st.session_state.admission_data.keys()):
                    st.header("ì…ì‹œê²°ê³¼")
                    for sheet_name, df_sheet in st.session_state.admission_data.items():
                        if "ì£¼ìš”ì‚¬í•­" in sheet_name:
                            continue
                        if selected_type != "ì „ì²´" and selected_type != sheet_name:
                            continue
                        st.markdown(f"**{sheet_name}**")
                        st.dataframe(wrap_long_text(df_sheet, max_len=50), use_container_width=True)

                    # ===== ì…ì‹œê²°ê³¼ Excel ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ =====
                    excel_buffer = BytesIO()
                    with pd.ExcelWriter(excel_buffer, engine="openpyxl") as writer:
                        for sheet_name, df_sheet in st.session_state.admission_data.items():
                            if selected_type != "ì „ì²´" and selected_type != sheet_name:
                                continue
                            df_sheet.to_excel(writer, sheet_name=sheet_name[:31], index=False, header=False)
                    excel_buffer.seek(0)
                    st.download_button(
                        label="ğŸ“¥ ì…ì‹œê²°ê³¼ ë‹¤ìš´ë¡œë“œ",
                        data=excel_buffer,
                        file_name=f"{sanitize_filename(selected_univ)}_{search_year-1}ë…„_ëŒ€í•™ì…ì‹œê²°ê³¼.xlsx",
                        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                    )

            # ===== PDF ë‹¤ìš´ë¡œë“œ =====
            with pdf_container:
                if st.session_state.pdf_buffers:
                    st.markdown("### ëª¨ì§‘ìš”ê°• PDF ë‹¤ìš´ë¡œë“œ")
                    for label, (content, fname) in st.session_state.pdf_buffers.items():
                        st.download_button(
                            label=f"ğŸ“„ {label} ë‹¤ìš´ë¡œë“œ",
                            data=content,
                            file_name=fname,
                            mime="application/pdf"
                        )
                else:
                    st.warning("ëª¨ì§‘ìš”ê°• PDFê°€ ì—†ìŠµë‹ˆë‹¤.")

            status_placeholder.success("í¬ë¡¤ë§ ì™„ë£Œ! âœ…")
